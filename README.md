Car Price Prediction using Ridge and Lasso Regression

This project demonstrates the application of regularization techniques â€” Ridge and Lasso Regression â€” to improve linear model performance and prevent overfitting.

ğŸš— Dataset
A synthetic car price dataset with 500 rows was generated.
It includes features such as engine size, horsepower, mileage, fuel type, transmission, owner type, etc.

âš™ï¸ Techniques Used
Linear Regression (baseline)
Ridge Regression â€“ adds L2 regularization to penalize large coefficients
Lasso Regression â€“ adds L1 regularization to shrink irrelevant features to zero

Steps followed:
Data Preprocessing â€“ handled categorical variables using pd.get_dummies().
Model Building â€“ trained three models: Linear, Ridge, and Lasso Regression.
Evaluation â€“ used metrics like r2_score, MSE, and RMSE for performance comparison.
Visualization â€“ created a comparative bar chart showing coefficient shrinkage and accuracy.

ğŸ“Š Results & Insights
Model	                Train Accuracy	Test Accuracy
Linear Regression	      ~97%	            ~96%
Ridge Regression	      ~98%	            ~97%
Lasso Regression	      ~96%	            ~95%

âœ… Ridge gives more stable predictions
âœ… Lasso simplifies the model by feature elimination

ğŸ“ˆ Visualization
The below chart compares model performance and coefficients.
<img width="695" height="480" alt="image" src="https://github.com/user-attachments/assets/8de96b16-f05e-4b50-8992-de58ce82fc04" />

ğŸ” Observation:
Linear Regression performed well but slightly overfitted.
Ridge Regression handled multicollinearity and improved generalization.
Lasso Regression performed feature selection, reducing less significant coefficients to zero.

ğŸ“š This project deepened my understanding of bias-variance trade-off and how regularization enhances model robustness.
